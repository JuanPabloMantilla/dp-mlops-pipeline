{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from scipy.stats import skew\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icustays = pd.read_csv(\"../data/mimic-iv-3.1/icu/icustays.csv\")\n",
    "admissions = pd.read_csv(\"../data/mimic-iv-3.1/hosp/admissions.csv\")\n",
    "patients = pd.read_csv(\"../data/mimic-iv-3.1/hosp/patients.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter patients under 18 years old\n",
    "\n",
    "icustays = icustays.merge(patients[['subject_id', 'anchor_age']], on='subject_id', how='left')\n",
    "icustays = icustays[icustays['anchor_age'] >= 18]\n",
    "print(f\"Number of adult ICU stays: {len(icustays)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out stays under 24 hours\n",
    "\n",
    "icustays = icustays[(icustays['los'] >= 1)]\n",
    "print(f\"Number of ICU stays with LOS over 1 day: {len(icustays)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter stays that resulted in death\n",
    "\n",
    "icustays = icustays.merge(admissions[['hadm_id', 'hospital_expire_flag']], on='hadm_id', how='left')\n",
    "icustays = icustays[icustays['hospital_expire_flag'] == 0]\n",
    "print(f\"Number of ICU stays excluding deaths: {len(icustays)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out multiple icu stays for single hospital admission\n",
    "\n",
    "icustays = icustays.drop_duplicates(subset='hadm_id', keep=False)\n",
    "icustays.to_csv('icustays_filtered.csv', index=False)\n",
    "print('Number of ICU stays excluding multiple admissions:', len(icustays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter labevents\n",
    "\n",
    "hadm_id_set = set(icustays['hadm_id'])\n",
    "\n",
    "chunksize = 50000\n",
    "\n",
    "filtered_chunks = []\n",
    "\n",
    "for chunk in pd.read_csv('../data/mimic-iv-3.1/hosp/labevents.csv', chunksize=chunksize):\n",
    "    filtered_chunk = chunk[chunk['hadm_id'].isin(hadm_id_set)]\n",
    "    filtered_chunks.append(filtered_chunk)\n",
    "\n",
    "filtered_labevents = pd.concat(filtered_chunks)\n",
    "filtered_labevents.to_csv('labevents_filtered.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter chartevents\n",
    "\n",
    "hadm_id_set = set(icustays['hadm_id'])\n",
    "\n",
    "chunksize = 50000\n",
    "\n",
    "output_file = 'chartevents_filtered.csv'\n",
    "first_chunk = True  \n",
    "\n",
    "for chunk in pd.read_csv('../data/mimic-iv-3.1/icu/chartevents.csv', chunksize=chunksize):\n",
    "    filtered_chunk = chunk[chunk['hadm_id'].isin(hadm_id_set)]\n",
    "\n",
    "    filtered_chunk.to_csv(output_file, mode='a', index=False, header=first_chunk)\n",
    "    first_chunk = False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract relevant chartevents based on itemid\n",
    "\n",
    "chartevents_itemids = {\n",
    "    \"Heart Rate (HR)\": 220045,\n",
    "    \"O2 Saturation (SpO2)\": 220277,\n",
    "    \"Respiratory Rate (RR)\": 220210,\n",
    "    \"Body Temperature (Fahrenheit)\": 223761,\n",
    "    \"GCS - Eye Opening\": 220739,\n",
    "    \"GCS - Verbal Response\": 223900,\n",
    "    \"GCS - Motor Response\": 223901\n",
    "}\n",
    "\n",
    "selected_itemids = [220045, 220277, 220210, 223761, 220739, 223900, 223901]\n",
    "\n",
    "chartevents_file = 'chartevents_filtered.csv'\n",
    "\n",
    "chunksize = 50000\n",
    "\n",
    "output_file = 'chartevents_filtered2.csv'\n",
    "\n",
    "write_header = True\n",
    "\n",
    "for chunk in pd.read_csv(chartevents_file, chunksize=chunksize, on_bad_lines='skip'):\n",
    "    filtered_chunk = chunk[chunk['itemid'].isin(selected_itemids)]\n",
    "    \n",
    "    filtered_chunk.to_csv(output_file, mode='a', index=False, header=write_header)\n",
    "    write_header = False\n",
    "\n",
    "print(f\"Filtered data has been saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract relevant labevents\n",
    "\n",
    "labevents_itemids = {\n",
    "    \"Anion Gap\": 50868,\n",
    "    \"Bicarbonate\": 50882,\n",
    "    \"Chloride\": 50902,\n",
    "    \"Creatinine\": 50912,\n",
    "    \"Glucose\": 50931,\n",
    "    \"Sodium\": 50983,\n",
    "    \"Magnesium\": 50960,\n",
    "    \"Potassium\": 50971,\n",
    "    \"Phosphate\": 50970,\n",
    "    \"Urea Nitrogen\": 51006,\n",
    "    \"Hematocrit\": 51221,\n",
    "    \"Hemoglobin\": 51222,\n",
    "    \"MCH\": 51248,\n",
    "    \"MCHC\": 51249,\n",
    "    \"MCV\": 51250,\n",
    "    \"Red Blood Cells\": 51279,\n",
    "    \"White Blood Cells\": 51301,\n",
    "    \"RDW\": 51277,\n",
    "    \"Platelets\": 51265\n",
    "}\n",
    "\n",
    "selected_itemids = [50868, 50882, 50902, 50912, 50931, 50983, 50960, 50971, 50970, \n",
    "                    51006, 51221, 51222, 51248, 51249, 51250, 51279, 51301, 51277, 51265]\n",
    "\n",
    "chartevents_file = 'labevents_filtered.csv'\n",
    "\n",
    "chunksize = 50000\n",
    "\n",
    "output_file = 'labevents_filtered2.csv'\n",
    "\n",
    "write_header = True\n",
    "\n",
    "for chunk in pd.read_csv(chartevents_file, chunksize=chunksize):\n",
    "    filtered_chunk = chunk[chunk['itemid'].isin(selected_itemids)]\n",
    "    \n",
    "    filtered_chunk.to_csv(output_file, mode='a', index=False, header=write_header)\n",
    "    write_header = False\n",
    "\n",
    "print(f\"Filtered data has been saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with intime and outtime\n",
    "\n",
    "icustays = pd.read_csv('icustays_filtered.csv')\n",
    "icu_times = icustays[['hadm_id', 'intime', 'outtime']]\n",
    "\n",
    "def process_large_merge(input_file, output_file, icu_times):\n",
    "    chunksize = 500000 \n",
    "    write_header = True\n",
    "    icu_times['hadm_id'] = icu_times['hadm_id'].astype(int)\n",
    "\n",
    "    for chunk in pd.read_csv(input_file, chunksize=chunksize, low_memory=False):\n",
    "        chunk = chunk.merge(icu_times, on='hadm_id', how='inner')  \n",
    "        chunk['hadm_id'] = chunk['hadm_id'].astype(int)\n",
    "        chunk.to_csv(output_file, mode='a', index=False, header=write_header) \n",
    "        write_header = False \n",
    "process_large_merge('chartevents_filtered2.csv', 'chartevents_times.csv', icu_times)\n",
    "\n",
    "process_large_merge('labevents_filtered2.csv', 'labevents_times.csv', icu_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chartevents = pd.read_csv('chartevents_times.csv', usecols=['hadm_id'])\n",
    "print('Chartevents, unique admissions:', chartevents['hadm_id'].nunique())\n",
    "\n",
    "labevents = pd.read_csv('labevents_times.csv', usecols=['hadm_id'])\n",
    "print('Labevents, unique admissions:', labevents['hadm_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only measurements within the first 24 hours\n",
    "\n",
    "def filter_dataframe_in_chunks(input_file, output_file):\n",
    "    chunksize = 50000\n",
    "    write_header = True \n",
    "\n",
    "    for chunk in pd.read_csv(input_file, chunksize=chunksize):\n",
    "        chunk['charttime'] = pd.to_datetime(chunk['charttime'])\n",
    "        chunk['intime'] = pd.to_datetime(chunk['intime'])\n",
    "\n",
    "        chunk['time_diff'] = (chunk['charttime'] - chunk['intime']).dt.total_seconds() / 3600\n",
    "\n",
    "        filtered_chunk = chunk[(chunk['time_diff'] >= 0) & (chunk['time_diff'] <= 24)]\n",
    "\n",
    "        filtered_chunk.to_csv(output_file, mode='a', index=False, header=write_header)\n",
    "        write_header = False  \n",
    "\n",
    "    print(f\"Filtered data saved to {output_file}.\")\n",
    "\n",
    "filter_dataframe_in_chunks('chartevents_times.csv', 'chartevents24.csv')\n",
    "\n",
    "filter_dataframe_in_chunks('labevents_times.csv', 'labevents24.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chartevents = pd.read_csv('chartevents24.csv', usecols=['hadm_id'])\n",
    "print('Chartevents, unique admissions:', chartevents['hadm_id'].nunique())\n",
    "print('Chartevents, length:', len(chartevents.index))\n",
    "\n",
    "labevents = pd.read_csv('labevents24.csv', usecols=['hadm_id'])\n",
    "print('Labevents, unique admissions:', labevents['hadm_id'].nunique())\n",
    "print('Labevents, length:', len(labevents.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists for chartevents and labevents\n",
    "\n",
    "def group_measurements(df):\n",
    "    df['value'] = df['valuenum'].fillna(df['value'])\n",
    "    grouped_df = df.groupby(['subject_id', 'hadm_id', 'itemid'])['value'].agg(list).reset_index()\n",
    "\n",
    "    return grouped_df\n",
    "\n",
    "chartevents = pd.read_csv('chartevents24.csv') \n",
    "chartevents_list = group_measurements(chartevents)\n",
    "chartevents_list.to_csv('chartevents_list.csv', index=False)\n",
    "\n",
    "labevents = pd.read_csv('labevents24.csv') \n",
    "labevents_list = group_measurements(labevents)\n",
    "labevents_list.to_csv('labevents_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for the % of missing itemids\n",
    "\n",
    "def calculate_missing_itemids_per_variable(labevents_file, chartevents_file, labevents_itemids, chartevents_itemids):\n",
    "\n",
    "  labevents_df = pd.read_csv(labevents_file)\n",
    "  chartevents_df = pd.read_csv(chartevents_file)\n",
    "\n",
    "  all_itemids = {**labevents_itemids, **chartevents_itemids} \n",
    "\n",
    "  missing_percentages = {}\n",
    "\n",
    "  for item_name, itemid in all_itemids.items():\n",
    "    if item_name in labevents_itemids:\n",
    "      filtered_df = labevents_df[labevents_df['itemid'] == labevents_itemids[item_name]]\n",
    "      total_admissions = labevents_df['hadm_id'].nunique()\n",
    "      admissions_with_item = filtered_df['hadm_id'].nunique()\n",
    "      missing_percentages[item_name] = ((total_admissions - admissions_with_item) / total_admissions) * 100\n",
    "    elif item_name in chartevents_itemids:\n",
    "      filtered_df = chartevents_df[chartevents_df['itemid'] == chartevents_itemids[item_name]]\n",
    "      total_admissions = chartevents_df['hadm_id'].nunique()\n",
    "      admissions_with_item = filtered_df['hadm_id'].nunique()\n",
    "      missing_percentages[item_name] = ((total_admissions - admissions_with_item) / total_admissions) * 100\n",
    "\n",
    "  return missing_percentages\n",
    "\n",
    "labevents_file = 'labevents_list.csv'\n",
    "chartevents_file = 'chartevents_list.csv'\n",
    "\n",
    "chartevents_itemids = {\n",
    "    \"Heart Rate (HR)\": 220045,\n",
    "    \"O2 Saturation (SpO2)\": 220277,\n",
    "    \"Respiratory Rate (RR)\": 220210,\n",
    "    \"Body Temperature (Fahrenheit)\": 223761,\n",
    "    \"GCS - Eye Opening\": 220739,\n",
    "    \"GCS - Verbal Response\": 223900,\n",
    "    \"GCS - Motor Response\": 223901\n",
    "}\n",
    "\n",
    "labevents_itemids = {\n",
    "    \"Anion Gap\": 50868,\n",
    "    \"Bicarbonate\": 50882,\n",
    "    \"Chloride\": 50902,\n",
    "    \"Creatinine\": 50912,\n",
    "    \"Glucose\": 50931,\n",
    "    \"Sodium\": 50983,\n",
    "    \"Magnesium\": 50960,\n",
    "    \"Potassium\": 50971,\n",
    "    \"Phosphate\": 50970,\n",
    "    \"Urea Nitrogen\": 51006,\n",
    "    \"Hematocrit\": 51221,\n",
    "    \"Hemoglobin\": 51222,\n",
    "    \"MCH\": 51248,\n",
    "    \"MCHC\": 51249,\n",
    "    \"MCV\": 51250,\n",
    "    \"Red Blood Cells\": 51279,\n",
    "    \"White Blood Cells\": 51301,\n",
    "    \"RDW\": 51277,\n",
    "    \"Platelets\": 51265\n",
    "}\n",
    "\n",
    "missing_percentages = calculate_missing_itemids_per_variable(labevents_file, chartevents_file, labevents_itemids, chartevents_itemids)\n",
    "\n",
    "for item_name, percentage in missing_percentages.items():\n",
    "  print(f\"{item_name}: {percentage:.2f}% missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge labevents and chartevents, only complete admissions\n",
    "\n",
    "def merge_and_filter_files(labevents_file, chartevents_file, labevents_itemids, chartevents_itemids):\n",
    "\n",
    "  labevents_df = pd.read_csv(labevents_file)\n",
    "  chartevents_df = pd.read_csv(chartevents_file)\n",
    "\n",
    "  labevents_filtered = labevents_df[labevents_df['itemid'].isin(labevents_itemids)]\n",
    "  chartevents_filtered = chartevents_df[chartevents_df['itemid'].isin(chartevents_itemids)]\n",
    "\n",
    "  labevents_grouped = labevents_filtered.groupby('hadm_id')['itemid'].nunique()\n",
    "  chartevents_grouped = chartevents_filtered.groupby('hadm_id')['itemid'].nunique()\n",
    "\n",
    "  labevents_complete_admissions = labevents_grouped[labevents_grouped == len(labevents_itemids)].index\n",
    "  chartevents_complete_admissions = chartevents_grouped[chartevents_grouped == len(chartevents_itemids)].index\n",
    "\n",
    "  complete_admissions = set(labevents_complete_admissions).intersection(set(chartevents_complete_admissions))\n",
    "\n",
    "  labevents_complete = labevents_filtered[labevents_filtered['hadm_id'].isin(complete_admissions)]\n",
    "  chartevents_complete = chartevents_filtered[chartevents_filtered['hadm_id'].isin(complete_admissions)]\n",
    "\n",
    "  merged_df = pd.concat([labevents_complete, chartevents_complete], ignore_index=True)\n",
    "\n",
    "  return merged_df\n",
    "\n",
    "labevents_file = 'labevents_list.csv' \n",
    "chartevents_file = 'chartevents_list.csv' \n",
    "labevents_itemids = [50868, 50882, 50902, 50912, 50931, 50983, 50960, 50971, 50970, \n",
    "                      51006, 51221, 51222, 51248, 51249, 51250, 51279, 51301, 51277, 51265]\n",
    "chartevents_itemids = [220045, 220277, 220210, 223761, 220739, 223900, 223901]\n",
    "\n",
    "merged_df = merge_and_filter_files(labevents_file, chartevents_file, \n",
    "                                   labevents_itemids, chartevents_itemids)\n",
    "\n",
    "merged_df.to_csv('measurements_complete.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements_complete = pd.read_csv('measurements_complete.csv')\n",
    "\n",
    "print('Number of measurements:', len(measurements_complete))\n",
    "print('Unique admission ids:', measurements_complete['hadm_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(measurements_complete.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add gender\n",
    "\n",
    "patients = pd.read_csv('data/mimic-iv-3.1/mimic-iv-3.1/hosp/patients.csv')\n",
    "measurements_complete = pd.read_csv('measurements_complete.csv')  \n",
    "\n",
    "merged = pd.merge(measurements_complete, patients[['subject_id','gender']], on='subject_id', how='left')\n",
    "\n",
    "merged.to_csv('data_with_gender.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add age, first careunit, LOS\n",
    "\n",
    "icustays = pd.read_csv('icustays_filtered.csv')\n",
    "data_with_gender = pd.read_csv('data_with_gender.csv')  \n",
    "\n",
    "merged = pd.merge(\n",
    "    data_with_gender,\n",
    "    icustays[['hadm_id', 'anchor_age', 'first_careunit', 'los']],\n",
    "    on='hadm_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "merged.to_csv('data_with_icustays_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add admission type, admission location\n",
    "\n",
    "admissions = pd.read_csv('data/mimic-iv-3.1/mimic-iv-3.1/hosp/admissions.csv')  \n",
    "\n",
    "data_with_admissions = pd.read_csv('data_with_icustays_data.csv') \n",
    "\n",
    "merged = pd.merge(\n",
    "    data_with_admissions,\n",
    "    admissions[['hadm_id', 'admission_type', 'admission_location']],\n",
    "    on='hadm_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "merged.to_csv('data_with_admissions_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add diagnoses\n",
    "\n",
    "diagnoses = pd.read_csv('data/mimic-iv-3.1/mimic-iv-3.1/hosp/diagnoses_icd.csv')\n",
    "\n",
    "data_with_admissions = pd.read_csv('data_with_admissions_data.csv') \n",
    "\n",
    "diagnoses_aggregated = (\n",
    "    diagnoses.groupby('hadm_id')\n",
    "    .apply(lambda x: [(code, version) for code, version in zip(x['icd_code'], x['icd_version'])]) \n",
    "    .reset_index()\n",
    "    .rename(columns={0: 'diagnoses_with_version'})\n",
    ")\n",
    "\n",
    "merged = pd.merge(data_with_admissions, diagnoses_aggregated, on='hadm_id', how='left')\n",
    "\n",
    "chunk_size = 100000  \n",
    "with open('data_with_diagnoses.csv', 'w') as f:\n",
    "    merged.iloc[:0].to_csv(f, index=False)\n",
    "    for start in range(0, len(merged), chunk_size):\n",
    "        merged.iloc[start:start + chunk_size].to_csv(f, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add los categories\n",
    "\n",
    "data_with_diagnoses = pd.read_csv('data_with_diagnoses.csv')  \n",
    "\n",
    "def categorize_los(los):\n",
    "    if los <= 3:\n",
    "        return 0\n",
    "    elif 3 < los <= 7:\n",
    "        return 1\n",
    "    elif 7 < los <= 14:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "data_with_diagnoses['los_class'] = data_with_diagnoses['los'].apply(categorize_los)\n",
    "\n",
    "data_with_diagnoses.to_csv('data_with_los.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out outliers\n",
    "\n",
    "df = pd.read_csv('data_with_los.csv')\n",
    "\n",
    "def filter_outliers(df):\n",
    "    df['value'] = df['value'].apply(ast.literal_eval)\n",
    "\n",
    "    expanded_df = df.explode('value')\n",
    "    expanded_df['value'] = pd.to_numeric(expanded_df['value'], errors='coerce')\n",
    "\n",
    "    iqr_bounds = expanded_df.groupby('itemid')['value'].agg(\n",
    "        q1=lambda x: x.quantile(0.25),\n",
    "        q3=lambda x: x.quantile(0.75)\n",
    "    )\n",
    "    iqr_bounds['iqr'] = iqr_bounds['q3'] - iqr_bounds['q1']\n",
    "    iqr_bounds['lower_bound'] = iqr_bounds['q1'] - 1.5 * iqr_bounds['iqr']\n",
    "    iqr_bounds['upper_bound'] = iqr_bounds['q3'] + 1.5 * iqr_bounds['iqr']\n",
    "\n",
    "    expanded_df = expanded_df.merge(iqr_bounds[['lower_bound', 'upper_bound']], on='itemid', how='left')\n",
    "\n",
    "    expanded_df = expanded_df[\n",
    "        (expanded_df['value'] >= expanded_df['lower_bound']) & \n",
    "        (expanded_df['value'] <= expanded_df['upper_bound'])\n",
    "    ]\n",
    "\n",
    "    filtered_values = expanded_df.groupby(['itemid', 'hadm_id'], as_index=False).agg({'value': list})\n",
    "\n",
    "    filtered_df = pd.merge(\n",
    "        df.drop(columns=['value']),\n",
    "        filtered_values,\n",
    "        on=['itemid', 'hadm_id'],\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "measurements = pd.read_csv('data_with_los.csv')\n",
    "print('loaded')\n",
    "\n",
    "filtered_measurements = filter_outliers(measurements)\n",
    "print('filtered')\n",
    "\n",
    "filtered_measurements.to_csv('data_without_outliers.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for complete admissions once more\n",
    "\n",
    "required_itemids = [\n",
    "    50868, 50882, 50902, 50912, 50931, 50983, 50960, 50971, 50970,\n",
    "    51006, 51221, 51222, 51248, 51249, 51250, 51279, 51301, 51277, 51265,\n",
    "    220045, 220277, 220210, 223761, 220739, 223900, 223901\n",
    "]\n",
    "\n",
    "data_without_outliers = pd.read_csv('data_without_outliers.csv')\n",
    "\n",
    "filtered_measurements = data_without_outliers[data_without_outliers['itemid'].isin(required_itemids)]\n",
    "\n",
    "hadm_itemid_count = (\n",
    "    filtered_measurements.groupby('hadm_id')['itemid']\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    "    .rename(columns={'itemid': 'unique_itemid_count'})\n",
    ")\n",
    "\n",
    "valid_hadm_ids = hadm_itemid_count[hadm_itemid_count['unique_itemid_count'] == len(required_itemids)]['hadm_id']\n",
    "\n",
    "final_filtered_measurements = data_without_outliers[data_without_outliers['hadm_id'].isin(valid_hadm_ids)]\n",
    "\n",
    "final_filtered_measurements.to_csv('data_complete.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate statistics\n",
    "\n",
    "measurements = pd.read_csv('data_complete.csv')\n",
    "\n",
    "def safe_eval(value):\n",
    "    try:\n",
    "        return literal_eval(value)  \n",
    "    except (ValueError, SyntaxError):\n",
    "        return [] \n",
    "\n",
    "measurements['value'] = measurements['value'].apply(safe_eval)\n",
    "\n",
    "measurements['value'] = measurements['value'].apply(lambda x: [v for v in x if not pd.isna(v)])\n",
    "\n",
    "measurements['min_value'] = measurements['value'].apply(lambda x: min(x) if x else np.nan)\n",
    "measurements['max_value'] = measurements['value'].apply(lambda x: max(x) if x else np.nan)\n",
    "measurements['mean_value'] = measurements['value'].apply(lambda x: np.mean(x) if x else np.nan)\n",
    "measurements['sd_value'] = measurements['value'].apply(lambda x: np.std(x, ddof=1) if len(x) > 1 else np.nan)\n",
    "measurements['skewness'] = measurements['value'].apply(lambda x: skew(x, bias=False) if len(x) > 1 else np.nan)\n",
    "measurements['num_observations'] = measurements['value'].apply(lambda x: len(x))\n",
    "\n",
    "measurements.to_csv('../data/variables_final.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
